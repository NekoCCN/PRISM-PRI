import torch
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import JSONResponse
from PIL import Image, ImageFile
import io
import os
import yaml
from torchvision import transforms
from typing import Optional
import traceback

from src.config import DEVICE, SERVER_CONFIG, STAGE2_CONFIG, STAGE1_CONFIG, DATA_YAML
from src.models.proposer import YOLOProposer
from src.models.refiner import ROIRefinerModel
from src.utils.postprocess import decode_refiner_output, non_max_suppression_global
import numpy as np

ImageFile.LOAD_TRUNCATED_IMAGES = True

app = FastAPI(
    title="PRISM çº§è”æ£€æµ‹ç³»ç»Ÿ API",
    description="ä¸Šä¼ é£åŠ›å‘ç”µæœºå¶ç‰‡å›¾ç‰‡ï¼Œè·å–æ™ºèƒ½ç¼ºé™·åˆ†ææŠ¥å‘Šã€‚",
    version="3.0.0"
)

# å…¨å±€å˜é‡
proposer = None
refiner = None
class_names = None
transform = None


@app.on_event("startup")
def load_models():
    """åœ¨æœåŠ¡å™¨å¯åŠ¨æ—¶åŠ è½½æ‰€æœ‰æ¨¡å‹"""
    global proposer, refiner, class_names, transform
    print("=" * 80)
    print("ğŸš€ æ­£åœ¨åŠ è½½PRISMçº§è”æ¨¡å‹...")
    print("=" * 80)

    # æ£€æŸ¥æƒé‡æ–‡ä»¶
    stage1_weights = STAGE1_CONFIG['weights_path']
    stage2_weights = SERVER_CONFIG.get('stage2_ema_weights', STAGE2_CONFIG['weights_path'])

    if not os.path.exists(stage1_weights):
        raise RuntimeError(f"âŒ æ‰¾ä¸åˆ°é˜¶æ®µä¸€æƒé‡: {stage1_weights}\nè¯·å…ˆè¿è¡Œ 'python main.py train-stage1'")

    if not os.path.exists(stage2_weights):
        print(f"âš ï¸  EMAæƒé‡ä¸å­˜åœ¨ï¼Œä½¿ç”¨ä¸»æ¨¡å‹: {STAGE2_CONFIG['weights_path']}")
        stage2_weights = STAGE2_CONFIG['weights_path']
        if not os.path.exists(stage2_weights):
            raise RuntimeError(f"âŒ æ‰¾ä¸åˆ°é˜¶æ®µäºŒæƒé‡: {stage2_weights}\nè¯·å…ˆè¿è¡Œ 'python main.py train-stage2-ultimate'")

    # åŠ è½½ç±»åˆ«ä¿¡æ¯
    with open(DATA_YAML, 'r') as f:
        data = yaml.safe_load(f)
        class_names = data['names']

    print(f"\nğŸ“‹ æ£€æµ‹ç±»åˆ«: {class_names}")

    # åŠ è½½é˜¶æ®µä¸€æ¨¡å‹
    print("\nğŸ”§ åŠ è½½é˜¶æ®µä¸€: YOLOæè®®ç½‘ç»œ...")
    proposer = YOLOProposer(weights_path=stage1_weights, device=DEVICE)
    print("   âœ… é˜¶æ®µä¸€åŠ è½½å®Œæˆ")

    # åŠ è½½é˜¶æ®µäºŒæ¨¡å‹
    print("\nğŸ”§ åŠ è½½é˜¶æ®µäºŒ: ROIç²¾ç‚¼ç½‘ç»œ...")
    refiner = ROIRefinerModel(device=DEVICE)

    checkpoint = torch.load(stage2_weights, map_location=DEVICE)
    if 'model_state_dict' in checkpoint:
        refiner.load_state_dict(checkpoint['model_state_dict'])
    else:
        refiner.load_state_dict(checkpoint)

    refiner.eval()
    print(f"   âœ… é˜¶æ®µäºŒåŠ è½½å®Œæˆ (ä½¿ç”¨{'EMA' if 'ema' in stage2_weights else 'ä¸»'}æ¨¡å‹)")

    # é¢„å¤„ç†å˜æ¢
    transform = transforms.Compose([
        transforms.Resize((STAGE2_CONFIG['roi_size'], STAGE2_CONFIG['roi_size'])),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

    print("\n" + "=" * 80)
    print("âœ… æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæ¯•ï¼ŒæœåŠ¡å™¨å‡†å¤‡å°±ç»ª")
    print("=" * 80)


@app.post("/predict/", summary="ç¼ºé™·æ£€æµ‹")
async def predict_defect(
        file: UploadFile = File(...),
        use_vlm: Optional[bool] = False
):
    """
    ä¸Šä¼ å›¾ç‰‡è¿›è¡Œç¼ºé™·æ£€æµ‹

    Args:
        file: å›¾ç‰‡æ–‡ä»¶
        use_vlm: æ˜¯å¦ä½¿ç”¨VLMè¿›è¡Œæ·±åº¦åˆ†æï¼ˆå¯é€‰ï¼‰

    Returns:
        æ£€æµ‹ç»“æœJSON
    """
    if not file.content_type.startswith('image/'):
        raise HTTPException(status_code=400, detail="âŒ æ–‡ä»¶ç±»å‹é”™è¯¯ï¼Œè¯·ä¸Šä¼ å›¾ç‰‡")

    try:
        # è¯»å–å›¾ç‰‡
        contents = await file.read()

        # ä¸´æ—¶ä¿å­˜ï¼ˆYOLOéœ€è¦æ–‡ä»¶è·¯å¾„ï¼‰
        temp_file_path = f"temp_{file.filename}"
        with open(temp_file_path, "wb") as f:
            f.write(contents)

        try:
            # ğŸ”¥ é˜¶æ®µä¸€ï¼šç”Ÿæˆå€™é€‰åŒºåŸŸ
            print(f"\nğŸ“¸ å¤„ç†å›¾ç‰‡: {file.filename}")
            print("   [é˜¶æ®µä¸€] ç”Ÿæˆå€™é€‰åŒºåŸŸ...")

            rois = proposer.propose(
                temp_file_path,
                tile_size=STAGE1_CONFIG['tile_size'],
                tile_overlap=STAGE1_CONFIG['tile_overlap'],
                conf_thresh=SERVER_CONFIG['proposer_confidence_threshold'],
                iou_thresh=SERVER_CONFIG['nms_iou_threshold']
            )

            print(f"   âœ… ç”Ÿæˆäº† {len(rois)} ä¸ªå€™é€‰åŒºåŸŸ")

            if rois.shape[0] == 0:
                return JSONResponse({
                    "filename": file.filename,
                    "status": "no_defects",
                    "message": "æœªæ£€æµ‹åˆ°æ½œåœ¨ç¼ºé™·",
                    "detections": []
                })

            # ğŸ”¥ é˜¶æ®µäºŒï¼šç²¾ç‚¼ROI
            print("   [é˜¶æ®µäºŒ] ç²¾ç‚¼å€™é€‰åŒºåŸŸ...")

            full_image = Image.open(io.BytesIO(contents)).convert("RGB")
            roi_batch = []

            for box in rois:
                # ç¡®ä¿åæ ‡æœ‰æ•ˆ
                x1, y1, x2, y2 = map(int, box)
                x1, y1 = max(0, x1), max(0, y1)
                x2, y2 = min(full_image.width, x2), min(full_image.height, y2)

                if x2 > x1 and y2 > y1:
                    roi_img = full_image.crop((x1, y1, x2, y2))
                    roi_batch.append(transform(roi_img))

            if len(roi_batch) == 0:
                return JSONResponse({
                    "filename": file.filename,
                    "status": "no_valid_rois",
                    "message": "æ— æœ‰æ•ˆå€™é€‰åŒºåŸŸ",
                    "detections": []
                })

            roi_tensors = torch.stack(roi_batch).to(DEVICE)

            with torch.no_grad():
                class_logits, bbox_deltas = refiner(roi_tensors)

            # ğŸ”¥ è§£ç ç»“æœ
            final_detections = []
            scores_tensor = torch.softmax(class_logits, dim=1)
            class_probs, class_preds = torch.max(scores_tensor, dim=1)

            for i in range(len(rois)):
                prob = class_probs[i].item()
                cls_id = class_preds[i].item()

                # è¿‡æ»¤èƒŒæ™¯å’Œä½ç½®ä¿¡åº¦
                if cls_id == (class_logits.shape[1] - 1) or prob < SERVER_CONFIG['refiner_confidence_threshold']:
                    continue

                # åº”ç”¨è¾¹ç•Œæ¡†å›å½’
                roi = rois[i]
                delta = bbox_deltas[i, cls_id * 4:(cls_id + 1) * 4].detach().cpu().numpy()

                w, h = roi[2] - roi[0], roi[3] - roi[1]
                cx, cy = roi[0] + 0.5 * w, roi[1] + 0.5 * h

                pred_cx = cx + delta[0] * w
                pred_cy = cy + delta[1] * h
                pred_w = w * np.exp(delta[2])
                pred_h = h * np.exp(delta[3])

                pred_x1 = float(pred_cx - 0.5 * pred_w)
                pred_y1 = float(pred_cy - 0.5 * pred_h)
                pred_x2 = float(pred_cx + 0.5 * pred_w)
                pred_y2 = float(pred_cy + 0.5 * pred_h)

                final_detections.append({
                    "class": class_names[cls_id],
                    "class_id": int(cls_id),
                    "confidence": float(prob),
                    "bbox": [pred_x1, pred_y1, pred_x2, pred_y2]
                })

            print(f"   âœ… æ£€æµ‹åˆ° {len(final_detections)} ä¸ªç¼ºé™·")

            # ğŸ”¥ å¯é€‰ï¼šVLMåˆ†æ
            vlm_analysis = None
            if use_vlm and len(final_detections) > 0:
                print("   [VLMåˆ†æ] ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š...")
                try:
                    from src.vlm import VisualLanguageModel
                    vlm = VisualLanguageModel()

                    # å¯¹ç¬¬ä¸€ä¸ªæ£€æµ‹åˆ°çš„ç¼ºé™·è¿›è¡ŒVLMåˆ†æ
                    det = final_detections[0]
                    roi_img = full_image.crop(det['bbox'])

                    vlm_analysis = vlm.analyze(
                        roi_img,
                        det['class'],
                        det['confidence'],
                        "ç¤ºä¾‹è®¾å¤‡å†å²æ•°æ®"
                    )
                    print("   âœ… VLMåˆ†æå®Œæˆ")
                except Exception as e:
                    print(f"   âš ï¸  VLMåˆ†æå¤±è´¥: {e}")

            return JSONResponse({
                "filename": file.filename,
                "status": "success",
                "num_detections": len(final_detections),
                "detections": final_detections,
                "vlm_analysis": vlm_analysis
            })

        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_file_path):
                os.remove(temp_file_path)

    except Exception as e:
        print(f"âŒ æ¨ç†é”™è¯¯: {e}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"æ¨ç†å¤±è´¥: {str(e)}")


@app.get("/", summary="æœåŠ¡å™¨çŠ¶æ€")
def read_root():
    """æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€"""
    return {
        "status": "running",
        "version": "3.0.0",
        "models_loaded": proposer is not None and refiner is not None,
        "device": str(DEVICE),
        "classes": class_names
    }


@app.get("/health", summary="å¥åº·æ£€æŸ¥")
def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {"status": "healthy"}
"""
æœ¬åœ°æ¨ç†è„šæœ¬
å•å¼ å›¾ç‰‡æˆ–æ‰¹é‡æ¨ç†
"""
import torch
import argparse
from pathlib import Path
from PIL import Image
import yaml
import json
from torchvision import transforms
from tqdm import tqdm
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np

from src.config import DEVICE, STAGE1_CONFIG, STAGE2_CONFIG, SERVER_CONFIG, DATA_YAML
from src.models.proposer import YOLOProposer
from src.models.refiner import ROIRefinerModel


class LocalInference:
    """æœ¬åœ°æ¨ç†å™¨"""

    def __init__(self, use_ema=True, use_tta=False):
        print("ğŸ”§ åˆå§‹åŒ–æ¨ç†å™¨...")

        # åŠ è½½ç±»åˆ«
        with open(DATA_YAML, 'r') as f:
            data = yaml.safe_load(f)
            self.class_names = data['names']

        # åŠ è½½æ¨¡å‹
        print("   [1/2] åŠ è½½é˜¶æ®µä¸€æ¨¡å‹...")
        self.proposer = YOLOProposer(
            weights_path=STAGE1_CONFIG['weights_path'],
            device=DEVICE
        )

        print("   [2/2] åŠ è½½é˜¶æ®µäºŒæ¨¡å‹...")
        self.refiner = ROIRefinerModel(device=DEVICE)

        # é€‰æ‹©æƒé‡
        if use_ema:
            weights_path = SERVER_CONFIG['stage2_ema_weights']
            if not Path(weights_path).exists():
                print(f"   âš ï¸  EMAæƒé‡ä¸å­˜åœ¨ï¼Œä½¿ç”¨ä¸»æ¨¡å‹")
                weights_path = STAGE2_CONFIG['weights_path']
        else:
            weights_path = STAGE2_CONFIG['weights_path']

        checkpoint = torch.load(weights_path, map_location=DEVICE)
        if 'model_state_dict' in checkpoint:
            self.refiner.load_state_dict(checkpoint['model_state_dict'])
        else:
            self.refiner.load_state_dict(checkpoint)

        self.refiner.eval()

        # TTA
        self.use_tta = use_tta
        if use_tta:
            from src.inference.tta import TestTimeAugmentation
            self.tta = TestTimeAugmentation(self.refiner)

        # é¢„å¤„ç†
        self.transform = transforms.Compose([
            transforms.Resize((STAGE2_CONFIG['roi_size'], STAGE2_CONFIG['roi_size'])),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])

        print("âœ… æ¨ç†å™¨åˆå§‹åŒ–å®Œæˆ\n")

    def predict_single(self, image_path, conf_thresh=0.5, save_viz=True):
        """å•å¼ å›¾ç‰‡æ¨ç†"""
        image_path = Path(image_path)

        if not image_path.exists():
            raise FileNotFoundError(f"å›¾ç‰‡ä¸å­˜åœ¨: {image_path}")

        print(f"ğŸ“¸ å¤„ç†å›¾ç‰‡: {image_path.name}")

        # é˜¶æ®µä¸€
        print("   [1/2] ç”Ÿæˆå€™é€‰åŒºåŸŸ...")
        rois = self.proposer.propose(
            str(image_path),
            tile_size=STAGE1_CONFIG['tile_size'],
            tile_overlap=STAGE1_CONFIG['tile_overlap'],
            conf_thresh=0.1
        )

        print(f"   ç”Ÿæˆ {len(rois)} ä¸ªå€™é€‰åŒºåŸŸ")

        if len(rois) == 0:
            print("   â„¹ï¸  æœªæ£€æµ‹åˆ°æ½œåœ¨ç¼ºé™·")
            return []

        # é˜¶æ®µäºŒ
        print("   [2/2] ç²¾ç‚¼æ£€æµ‹...")
        full_image = Image.open(image_path).convert("RGB")
        roi_batch = []
        valid_rois = []

        for box in rois:
            x1, y1, x2, y2 = map(int, box)
            x1, y1 = max(0, x1), max(0, y1)
            x2 = min(full_image.width, x2)
            y2 = min(full_image.height, y2)

            if x2 > x1 and y2 > y1:
                roi_img = full_image.crop((x1, y1, x2, y2))
                roi_batch.append(self.transform(roi_img))
                valid_rois.append([x1, y1, x2, y2])

        if len(roi_batch) == 0:
            print("   â„¹ï¸  æ— æœ‰æ•ˆå€™é€‰åŒºåŸŸ")
            return []

        roi_tensors = torch.stack(roi_batch).to(DEVICE)

        with torch.no_grad():
            if self.use_tta:
                # ä½¿ç”¨TTA
                class_logits_list = []
                bbox_deltas_list = []
                for roi_tensor in roi_tensors:
                    cls, reg = self.tta.predict_with_tta(roi_tensor)
                    class_logits_list.append(cls)
                    bbox_deltas_list.append(reg)
                class_logits = torch.cat(class_logits_list, dim=0)
                bbox_deltas = torch.cat(bbox_deltas_list, dim=0)
            else:
                class_logits, bbox_deltas = self.refiner(roi_tensors)

        # è§£ç 
        detections = []
        scores = torch.softmax(class_logits, dim=1)
        class_probs, class_preds = torch.max(scores, dim=1)

        for i, roi in enumerate(valid_rois):
            prob = class_probs[i].item()
            cls_id = class_preds[i].item()

            if cls_id == (class_logits.shape[1] - 1) or prob < conf_thresh:
                continue

            # è¾¹ç•Œæ¡†å›å½’
            delta = bbox_deltas[i, cls_id * 4:(cls_id + 1) * 4].cpu().numpy()
            w, h = roi[2] - roi[0], roi[3] - roi[1]
            cx, cy = roi[0] + 0.5 * w, roi[1] + 0.5 * h

            pred_cx = cx + delta[0] * w
            pred_cy = cy + delta[1] * h
            pred_w = w * np.exp(delta[2])
            pred_h = h * np.exp(delta[3])

            pred_x1 = pred_cx - 0.5 * pred_w
            pred_y1 = pred_cy - 0.5 * pred_h
            pred_x2 = pred_cx + 0.5 * pred_w
            pred_y2 = pred_cy + 0.5 * pred_h

            detections.append({
                "class": self.class_names[cls_id],
                "class_id": int(cls_id),
                "confidence": float(prob),
                "bbox": [float(pred_x1), float(pred_y1), float(pred_x2), float(pred_y2)]
            })

        print(f"   âœ… æ£€æµ‹åˆ° {len(detections)} ä¸ªç¼ºé™·")

        # å¯è§†åŒ–
        if save_viz and len(detections) > 0:
            self._visualize(full_image, detections, image_path)

        return detections

    def predict_batch(self, image_dir, output_json="results.json", conf_thresh=0.5):
        """æ‰¹é‡æ¨ç†"""
        image_dir = Path(image_dir)
        image_files = list(image_dir.glob("*.jpg")) + list(image_dir.glob("*.png"))

        if len(image_files) == 0:
            print(f"âŒ æœªæ‰¾åˆ°å›¾ç‰‡: {image_dir}")
            return

        print(f"ğŸ“ æ‰¹é‡å¤„ç† {len(image_files)} å¼ å›¾ç‰‡...")

        results = {}
        for img_path in tqdm(image_files, desc="æ¨ç†è¿›åº¦"):
            detections = self.predict_single(img_path, conf_thresh, save_viz=False)
            results[img_path.name] = detections

        # ä¿å­˜ç»“æœ
        with open(output_json, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        print(f"\nâœ… ç»“æœå·²ä¿å­˜: {output_json}")

        # ç»Ÿè®¡
        total_detections = sum(len(v) for v in results.values())
        print(f"\nğŸ“Š ç»Ÿè®¡:")
        print(f"   - æ€»å›¾ç‰‡æ•°: {len(results)}")
        print(f"   - æ€»æ£€æµ‹æ•°: {total_detections}")
        print(f"   - å¹³å‡æ¯å¼ : {total_detections / len(results):.2f}")

    def _visualize(self, image, detections, save_path):
        """å¯è§†åŒ–ç»“æœ"""
        fig, ax = plt.subplots(1, figsize=(12, 8))
        ax.imshow(image)

        colors = plt.cm.tab10(np.linspace(0, 1, len(self.class_names)))

        for det in detections:
            x1, y1, x2, y2 = det['bbox']
            cls_id = det['class_id']

            rect = patches.Rectangle(
                (x1, y1), x2 - x1, y2 - y1,
                linewidth=2,
                edgecolor=colors[cls_id],
                facecolor='none'
            )
            ax.add_patch(rect)

            label = f"{det['class']}: {det['confidence']:.2f}"
            ax.text(
                x1, y1 - 5,
                label,
                color='white',
                fontsize=10,
                bbox=dict(facecolor=colors[cls_id], alpha=0.8)
            )

        ax.axis('off')

        output_path = save_path.parent / f"{save_path.stem}_result.png"
        plt.savefig(output_path, bbox_inches='tight', dpi=150)
        plt.close()

        print(f"   ğŸ’¾ å¯è§†åŒ–å·²ä¿å­˜: {output_path}")


def main():
    parser = argparse.ArgumentParser(description="PRISMæœ¬åœ°æ¨ç†")
    parser.add_argument('--image', type=str, help="å•å¼ å›¾ç‰‡è·¯å¾„")
    parser.add_argument('--dir', type=str, help="å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆæ‰¹é‡ï¼‰")
    parser.add_argument('--conf', type=float, default=0.5, help="ç½®ä¿¡åº¦é˜ˆå€¼")
    parser.add_argument('--ema', action='store_true', help="ä½¿ç”¨EMAæ¨¡å‹")
    parser.add_argument('--tta', action='store_true', help="ä½¿ç”¨TTA")
    parser.add_argument('--output', type=str, default='results.json', help="è¾“å‡ºJSONè·¯å¾„")

    args = parser.parse_args()

    if not args.image and not args.dir:
        parser.error("è¯·æŒ‡å®š --image æˆ– --dir")

    # åˆå§‹åŒ–
    inferencer = LocalInference(use_ema=args.ema, use_tta=args.tta)

    if args.image:
        # å•å¼ æ¨ç†
        detections = inferencer.predict_single(args.image, conf_thresh=args.conf)
        print(f"\næ£€æµ‹ç»“æœ:")
        print(json.dumps(detections, indent=2, ensure_ascii=False))

    elif args.dir:
        # æ‰¹é‡æ¨ç†
        inferencer.predict_batch(args.dir, output_json=args.output, conf_thresh=args.conf)


if __name__ == '__main__':
    main()
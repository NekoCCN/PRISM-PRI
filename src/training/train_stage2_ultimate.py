# src/training/train_stage2_ultimate.py
"""
é˜¶æ®µäºŒç»ˆæä¼˜åŒ–è®­ç»ƒè„šæœ¬

åŒ…å«ä¼˜åŒ–ï¼š
1. OHEM + Focal Loss
2. Balanced L1 Loss
3. EMA (æŒ‡æ•°ç§»åŠ¨å¹³å‡)
4. SWA (éšæœºæƒé‡å¹³å‡)
5. æ··åˆç²¾åº¦è®­ç»ƒ
6. æ¢¯åº¦è£å‰ª
7. Warmup + Cosineå­¦ä¹ ç‡è°ƒåº¦
8. åŠ¨æ€æŸå¤±æƒé‡
9. æ—©åœæœºåˆ¶
10. é«˜çº§æ•°æ®å¢å¼º
"""
import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import autocast, GradScaler
from torch.utils.data import DataLoader
from torchvision import transforms
from tqdm import tqdm
import numpy as np
import os
import yaml

from src.dataset import ROIDataset
from src.models.refiner import ROIRefinerModel
from src.config import DEVICE, STAGE2_CONFIG, DATASET_DIR, WEIGHTS_DIR
from src.training.losses import OHEMFocalLoss, BalancedL1Loss
from src.training.ema import ModelEMA, SWA


class WarmupCosineScheduler:
    """Warmup + Cosine Annealing å­¦ä¹ ç‡è°ƒåº¦å™¨"""

    def __init__(self, optimizer, warmup_epochs, total_epochs, min_lr=1e-6):
        self.optimizer = optimizer
        self.warmup_epochs = warmup_epochs
        self.total_epochs = total_epochs
        self.min_lr = min_lr
        self.base_lr = optimizer.param_groups[0]['lr']

    def step(self, epoch):
        if epoch < self.warmup_epochs:
            # Warmupé˜¶æ®µ
            lr = self.base_lr * (epoch + 1) / self.warmup_epochs
        else:
            # Cosineè¡°å‡
            progress = (epoch - self.warmup_epochs) / (self.total_epochs - self.warmup_epochs)
            lr = self.min_lr + (self.base_lr - self.min_lr) * 0.5 * (1 + np.cos(np.pi * progress))

        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
        return lr


class EarlyStopping:
    """æ—©åœæœºåˆ¶"""

    def __init__(self, patience=10, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False

    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
                return True
        else:
            self.best_loss = val_loss
            self.counter = 0
        return False


def run_training_stage2_ultimate():
    """ç»ˆæä¼˜åŒ–ç‰ˆè®­ç»ƒå…¥å£"""
    print("=" * 80)
    print("ğŸš€ é˜¶æ®µäºŒè®­ç»ƒ - ç»ˆæä¼˜åŒ–ç‰ˆ")
    print("=" * 80)
    print("\nä¼˜åŒ–ç‰¹æ€§:")
    print("  âœ… OHEM + Focal Loss (è§£å†³ç±»åˆ«ä¸å¹³è¡¡)")
    print("  âœ… Balanced L1 Loss (æ”¹è¿›å›å½’)")
    print("  âœ… EMA (æå‡æ³›åŒ–èƒ½åŠ›)")
    print("  âœ… SWA (è®­ç»ƒåæœŸå¯ç”¨)")
    print("  âœ… æ··åˆç²¾åº¦è®­ç»ƒ (åŠ é€Ÿ1.5-2å€)")
    print("  âœ… æ¢¯åº¦è£å‰ª (ç¨³å®šè®­ç»ƒ)")
    print("  âœ… Warmup + Cosine LR")
    print("  âœ… åŠ¨æ€æŸå¤±æƒé‡")
    print("  âœ… æ—©åœæœºåˆ¶")
    print("  âœ… å¢å¼ºæ•°æ®å¢å¼º")

    # ==================== æ•°æ®å‡†å¤‡ ====================
    print("\n" + "=" * 80)
    print("ğŸ“¦ æ­¥éª¤1: å‡†å¤‡æ•°æ®é›†")
    print("=" * 80)

    train_transform = transforms.Compose([
        transforms.Resize((STAGE2_CONFIG['roi_size'], STAGE2_CONFIG['roi_size'])),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomVerticalFlip(p=0.5),
        transforms.RandomRotation(15),
        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),
        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

    train_dataset = ROIDataset(
        proposals_file=STAGE2_CONFIG['proposals_json'],
        transform=train_transform,
        positive_thresh=STAGE2_CONFIG['positive_iou_thresh'],
        negative_thresh=STAGE2_CONFIG['negative_iou_thresh']
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=STAGE2_CONFIG['batch_size'],
        shuffle=True,
        num_workers=4,
        pin_memory=True,
        drop_last=True
    )

    print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ")
    print(f"   - æ ·æœ¬æ•°é‡: {len(train_dataset)}")
    print(f"   - Batchå¤§å°: {STAGE2_CONFIG['batch_size']}")
    print(f"   - æ€»batchæ•°: {len(train_loader)}")

    # ==================== æ¨¡å‹åˆå§‹åŒ– ====================
    print("\n" + "=" * 80)
    print("ğŸ—ï¸  æ­¥éª¤2: åˆå§‹åŒ–æ¨¡å‹")
    print("=" * 80)

    # è¯»å–ç±»åˆ«æ•°
    with open(os.path.join(DATASET_DIR, 'data.yaml'), 'r') as f:
        num_classes = yaml.safe_load(f)['nc']

    model = ROIRefinerModel(device=DEVICE)

    # ğŸ”¥ EMAæ¨¡å‹
    ema = ModelEMA(model, decay=0.9999)

    # ğŸ”¥ SWAæ¨¡å‹ï¼ˆè®­ç»ƒåæœŸå¯ç”¨ï¼‰
    swa = SWA(model)
    swa_start_epoch = int(STAGE2_CONFIG['epochs'] * 0.75)

    print(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    print(f"   - ç±»åˆ«æ•°: {num_classes}")
    print(f"   - EMA decay: 0.9999")
    print(f"   - SWAå¯åŠ¨: ç¬¬{swa_start_epoch}ä¸ªepoch")

    # ==================== æŸå¤±å‡½æ•° ====================
    print("\n" + "=" * 80)
    print("ğŸ“Š æ­¥éª¤3: é…ç½®æŸå¤±å‡½æ•°")
    print("=" * 80)

    loss_classifier = OHEMFocalLoss(
        alpha=0.25,
        gamma=2.0,
        ohem_ratio=0.7
    )

    loss_regressor = BalancedL1Loss(
        alpha=0.5,
        gamma=1.5,
        beta=1.0
    )

    print("âœ… æŸå¤±å‡½æ•°é…ç½®å®Œæˆ")
    print("   - åˆ†ç±»: OHEM + Focal Loss")
    print("   - å›å½’: Balanced L1 Loss")

    # ==================== ä¼˜åŒ–å™¨ ====================
    print("\n" + "=" * 80)
    print("âš™ï¸  æ­¥éª¤4: é…ç½®ä¼˜åŒ–å™¨")
    print("=" * 80)

    optimizer = optim.AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=STAGE2_CONFIG['learning_rate'],
        weight_decay=1e-4,
        betas=(0.9, 0.999)
    )

    scheduler = WarmupCosineScheduler(
        optimizer,
        warmup_epochs=5,
        total_epochs=STAGE2_CONFIG['epochs'],
        min_lr=1e-6
    )

    print("âœ… ä¼˜åŒ–å™¨é…ç½®å®Œæˆ")
    print(f"   - ç±»å‹: AdamW")
    print(f"   - åˆå§‹å­¦ä¹ ç‡: {STAGE2_CONFIG['learning_rate']}")
    print(f"   - Warmup: 5 epochs")

    # ==================== å…¶ä»–ç»„ä»¶ ====================
    scaler = GradScaler()  # æ··åˆç²¾åº¦
    early_stopping = EarlyStopping(patience=15, min_delta=0.001)

    # ==================== è®­ç»ƒå¾ªç¯ ====================
    print("\n" + "=" * 80)
    print("ğŸ¯ æ­¥éª¤5: å¼€å§‹è®­ç»ƒ")
    print("=" * 80)

    best_loss = float('inf')
    loss_history = {'cls': [], 'reg': []}
    loss_weights = {'cls': 1.0, 'reg': 1.0}

    for epoch in range(STAGE2_CONFIG['epochs']):
        model.train()
        epoch_cls_loss = 0
        epoch_reg_loss = 0
        epoch_total_loss = 0
        num_batches = 0

        current_lr = scheduler.step(epoch)

        pbar = tqdm(
            train_loader,
            desc=f"Epoch {epoch + 1}/{STAGE2_CONFIG['epochs']}",
            ncols=120
        )

        for roi_images, class_labels, reg_targets in pbar:
            roi_images = roi_images.to(DEVICE)
            class_labels = class_labels.to(DEVICE)
            reg_targets = reg_targets.to(DEVICE)

            cls_mask = class_labels != -2
            pos_mask = class_labels >= 0

            # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
            with autocast():
                class_logits, bbox_deltas = model(roi_images)

                # åˆ†ç±»æŸå¤±
                if cls_mask.sum() > 0:
                    cls_loss, ohem_mask = loss_classifier(
                        class_logits[cls_mask],
                        class_labels[cls_mask] + 1
                    )
                else:
                    cls_loss = torch.tensor(0.0, device=DEVICE)
                    ohem_mask = torch.zeros(1, dtype=torch.bool, device=DEVICE)

                # å›å½’æŸå¤±
                if pos_mask.sum() > 0:
                    bbox_deltas_pos = bbox_deltas[pos_mask]
                    class_labels_pos = class_labels[pos_mask]
                    indices = torch.arange(len(class_labels_pos), device=DEVICE)
                    selected_deltas = bbox_deltas_pos.view(
                        -1, num_classes, 4
                    )[indices, class_labels_pos.long()]

                    reg_loss = loss_regressor(selected_deltas, reg_targets[pos_mask])
                else:
                    reg_loss = torch.tensor(0.0, device=DEVICE)

                # åŠ¨æ€è°ƒæ•´æŸå¤±æƒé‡
                if len(loss_history['cls']) > 20:
                    avg_cls = np.mean(loss_history['cls'][-20:])
                    avg_reg = np.mean(loss_history['reg'][-20:])
                    if avg_reg > 1e-6:
                        ratio = avg_cls / avg_reg
                        loss_weights['reg'] = np.clip(ratio, 0.5, 2.0)

                total_loss = (loss_weights['cls'] * cls_loss +
                              loss_weights['reg'] * reg_loss)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            scaler.scale(total_loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()

            # æ›´æ–°EMA
            ema.update(model)

            # æ›´æ–°SWA (åæœŸ)
            if epoch >= swa_start_epoch:
                swa.update(model)

            # è®°å½•
            epoch_cls_loss += cls_loss.item() if isinstance(cls_loss, torch.Tensor) else 0
            epoch_reg_loss += reg_loss.item() if isinstance(reg_loss, torch.Tensor) else 0
            epoch_total_loss += total_loss.item()
            num_batches += 1

            loss_history['cls'].append(cls_loss.item() if isinstance(cls_loss, torch.Tensor) else 0)
            loss_history['reg'].append(reg_loss.item() if isinstance(reg_loss, torch.Tensor) else 0)

            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'LR': f'{current_lr:.2e}',
                'Loss': f'{total_loss.item():.3f}',
                'Cls': f'{cls_loss.item():.3f}' if isinstance(cls_loss, torch.Tensor) else '0',
                'Reg': f'{reg_loss.item():.3f}' if isinstance(reg_loss, torch.Tensor) else '0'
            })

        # Epochç»Ÿè®¡
        avg_cls = epoch_cls_loss / num_batches
        avg_reg = epoch_reg_loss / num_batches
        avg_total = epoch_total_loss / num_batches

        print(f"\nğŸ“Š Epoch {epoch + 1} ç»Ÿè®¡:")
        print(f"   - æ€»æŸå¤±: {avg_total:.4f}")
        print(f"   - åˆ†ç±»æŸå¤±: {avg_cls:.4f}")
        print(f"   - å›å½’æŸå¤±: {avg_reg:.4f}")
        print(f"   - å­¦ä¹ ç‡: {current_lr:.2e}")
        print(f"   - å›å½’æƒé‡: {loss_weights['reg']:.2f}")

        # æ—©åœæ£€æŸ¥
        if early_stopping(avg_total):
            print(f"\nâš ï¸  æ—©åœè§¦å‘ï¼è®­ç»ƒåœ¨ç¬¬ {epoch + 1} epochåœæ­¢")
            break

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_total < best_loss:
            best_loss = avg_total

            # ä¿å­˜ä¸»æ¨¡å‹
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': best_loss,
            }, STAGE2_CONFIG['weights_path'])

            # ä¿å­˜EMAæ¨¡å‹
            ema_path = STAGE2_CONFIG['weights_path'].replace('.pth', '_ema.pth')
            torch.save({
                'epoch': epoch,
                'model_state_dict': ema.ema.state_dict(),
                'ema_state': ema.state_dict(),
                'loss': best_loss,
            }, ema_path)

            print(f"   âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (Loss: {best_loss:.4f})")

    # ==================== ä¿å­˜SWAæ¨¡å‹ ====================
    if epoch >= swa_start_epoch:
        swa_path = STAGE2_CONFIG['weights_path'].replace('.pth', '_swa.pth')
        torch.save(swa.state_dict(), swa_path)
        print(f"\nâœ… SWAæ¨¡å‹å·²ä¿å­˜: {swa_path}")

    # ==================== è®­ç»ƒå®Œæˆ ====================
    print("\n" + "=" * 80)
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print("=" * 80)
    print(f"\nğŸ“Š æœ€ç»ˆç»“æœ:")
    print(f"   - æœ€ä½³æŸå¤±: {best_loss:.4f}")
    print(f"   - ä¸»æ¨¡å‹: {STAGE2_CONFIG['weights_path']}")
    print(f"   - EMAæ¨¡å‹: {ema_path}")
    if epoch >= swa_start_epoch:
        print(f"   - SWAæ¨¡å‹: {swa_path}")

    print(f"\nğŸ’¡ æ¨è:")
    print(f"   1. ä½¿ç”¨EMAæ¨¡å‹è¿›è¡Œæ¨ç†ï¼ˆé€šå¸¸æ¯”ä¸»æ¨¡å‹é«˜1-2%ï¼‰")
    print(f"   2. å¦‚æœå¯ç”¨äº†SWAï¼Œä¹Ÿå¯å°è¯•SWAæ¨¡å‹")


if __name__ == '__main__':
    run_training_stage2_ultimate()
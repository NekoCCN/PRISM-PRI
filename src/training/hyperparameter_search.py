"""
è‡ªåŠ¨è¶…å‚æ•°æœç´¢
ä½¿ç”¨Optunaæ‰¾åˆ°æœ€ä½³é…ç½®
"""
import optuna
from optuna.trial import Trial
import torch
from src.training.train_stage2_ultimate import *


def objective(trial: Trial):
    """
    Optunaç›®æ ‡å‡½æ•°

    æœç´¢ç©ºé—´ï¼š
    - learning_rate
    - batch_size
    - ohem_ratio
    - focal_alpha/gamma
    - warmup_epochs
    """

    # å®šä¹‰æœç´¢ç©ºé—´
    config = {
        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True),
        'batch_size': trial.suggest_categorical('batch_size', [4, 8, 16]),
        'ohem_ratio': trial.suggest_float('ohem_ratio', 0.5, 0.9),
        'focal_alpha': trial.suggest_float('focal_alpha', 0.1, 0.5),
        'focal_gamma': trial.suggest_float('focal_gamma', 1.0, 3.0),
        'warmup_epochs': trial.suggest_int('warmup_epochs', 3, 10),
        'weight_decay': trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True),
    }

    # æ›´æ–°å…¨å±€é…ç½®
    STAGE2_CONFIG.update(config)

    # è¿è¡Œè®­ç»ƒï¼ˆç¼©çŸ­ç‰ˆï¼‰
    STAGE2_CONFIG['epochs'] = 20  # å¿«é€Ÿè¯•éªŒ

    try:
        # æ‰§è¡Œè®­ç»ƒ
        best_loss = run_training_stage2_ultimate_for_optuna(trial, config)
        return best_loss

    except Exception as e:
        print(f"Trial failed: {e}")
        return float('inf')


def run_training_stage2_ultimate_for_optuna(trial, config):
    """
    ä¸ºOptunaå®šåˆ¶çš„è®­ç»ƒå‡½æ•°
    æ·»åŠ ä¸­é—´æŠ¥å‘Šå’Œå‰ªæ
    """
    # ... ç®€åŒ–çš„è®­ç»ƒä»£ç  ...

    for epoch in range(20):
        # è®­ç»ƒä¸€ä¸ªepoch
        avg_loss = train_one_epoch()

        # æŠ¥å‘Šä¸­é—´ç»“æœ
        trial.report(avg_loss, epoch)

        # å‰ªæï¼šå¦‚æœæ•ˆæœå¾ˆå·®å°±æå‰åœæ­¢
        if trial.should_prune():
            raise optuna.TrialPruned()

    return best_loss


def search_hyperparameters(n_trials=50):
    """
    æ‰§è¡Œè¶…å‚æ•°æœç´¢

    Args:
        n_trials: è¯•éªŒæ¬¡æ•°
    """
    print(f"ğŸ” å¼€å§‹è¶…å‚æ•°æœç´¢ ({n_trials} trials)...")

    # åˆ›å»ºstudy
    study = optuna.create_study(
        study_name='prism-stage2-hp-search',
        direction='minimize',
        pruner=optuna.pruners.MedianPruner(n_startup_trials=5),
        sampler=optuna.samplers.TPESampler()
    )

    # æ‰§è¡Œæœç´¢
    study.optimize(objective, n_trials=n_trials, timeout=3600 * 24)  # 24å°æ—¶

    # æ‰“å°ç»“æœ
    print("\n" + "=" * 80)
    print("ğŸ‰ æœç´¢å®Œæˆï¼")
    print("=" * 80)
    print(f"\næœ€ä½³Trial: {study.best_trial.number}")
    print(f"æœ€ä½³Loss: {study.best_value:.4f}")
    print(f"\næœ€ä½³è¶…å‚æ•°:")
    for key, value in study.best_params.items():
        print(f"  {key}: {value}")

    # å¯è§†åŒ–
    try:
        import matplotlib.pyplot as plt

        # ä¼˜åŒ–å†å²
        fig = optuna.visualization.matplotlib.plot_optimization_history(study)
        fig.savefig('hp_search_history.png')

        # å‚æ•°é‡è¦æ€§
        fig = optuna.visualization.matplotlib.plot_param_importances(study)
        fig.savefig('hp_param_importance.png')

        print("\nå¯è§†åŒ–å·²ä¿å­˜")
    except:
        pass

    return study.best_params


# ä½¿ç”¨
if __name__ == '__main__':
    best_params = search_hyperparameters(n_trials=50)

    # ä¿å­˜æœ€ä½³é…ç½®
    import json

    with open('best_hyperparameters.json', 'w') as f:
        json.dump(best_params, f, indent=2)
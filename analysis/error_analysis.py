"""
é”™è¯¯æ¡ˆä¾‹åˆ†æå·¥å…·
æ‰¾å‡ºæ¨¡å‹çš„å¼±ç‚¹
"""
import json
import numpy as np
from pathlib import Path
from collections import defaultdict
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report


class ErrorAnalyzer:
    """
    é”™è¯¯åˆ†æå™¨

    åŠŸèƒ½ï¼š
    1. æ··æ·†çŸ©é˜µ
    2. é”™è¯¯åˆ†å¸ƒ
    3. éš¾ä¾‹æŒ–æ˜
    4. æ€§èƒ½ç“¶é¢ˆåˆ†æ
    """

    def __init__(self, class_names):
        self.class_names = class_names
        self.errors = defaultdict(list)

    def add_prediction(self, image_id, pred_class, true_class, confidence, bbox):
        """è®°å½•ä¸€æ¬¡é¢„æµ‹"""
        is_correct = (pred_class == true_class)

        self.errors['image_id'].append(image_id)
        self.errors['pred_class'].append(pred_class)
        self.errors['true_class'].append(true_class)
        self.errors['confidence'].append(confidence)
        self.errors['is_correct'].append(is_correct)
        self.errors['bbox'].append(bbox)

    def generate_report(self, output_dir='error_analysis'):
        """ç”Ÿæˆå®Œæ•´çš„é”™è¯¯åˆ†ææŠ¥å‘Š"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)

        print("ğŸ“Š ç”Ÿæˆé”™è¯¯åˆ†ææŠ¥å‘Š...")

        # 1. æ··æ·†çŸ©é˜µ
        self._plot_confusion_matrix(output_dir)

        # 2. é”™è¯¯åˆ†å¸ƒ
        self._plot_error_distribution(output_dir)

        # 3. ç½®ä¿¡åº¦åˆ†æ
        self._analyze_confidence(output_dir)

        # 4. éš¾ä¾‹æŒ–æ˜
        hard_cases = self._find_hard_cases()

        # 5. ç”ŸæˆJSONæŠ¥å‘Š
        report = {
            "total_predictions": len(self.errors['image_id']),
            "accuracy": sum(self.errors['is_correct']) / len(self.errors['image_id']),
            "hard_cases": hard_cases,
            "per_class_stats": self._compute_per_class_stats()
        }

        with open(output_dir / 'report.json', 'w') as f:
            json.dump(report, f, indent=2)

        print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_dir}")
        return report

    def _plot_confusion_matrix(self, output_dir):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        y_true = self.errors['true_class']
        y_pred = self.errors['pred_class']

        cm = confusion_matrix(y_true, y_pred)

        plt.figure(figsize=(10, 8))
        sns.heatmap(
            cm,
            annot=True,
            fmt='d',
            cmap='Blues',
            xticklabels=self.class_names,
            yticklabels=self.class_names
        )
        plt.title('æ··æ·†çŸ©é˜µ')
        plt.ylabel('çœŸå®ç±»åˆ«')
        plt.xlabel('é¢„æµ‹ç±»åˆ«')
        plt.tight_layout()
        plt.savefig(output_dir / 'confusion_matrix.png', dpi=150)
        plt.close()

        # åˆ†ç±»æŠ¥å‘Š
        report = classification_report(
            y_true, y_pred,
            target_names=self.class_names,
            output_dict=True
        )

        with open(output_dir / 'classification_report.json', 'w') as f:
            json.dump(report, f, indent=2)

    def _plot_error_distribution(self, output_dir):
        """é”™è¯¯åˆ†å¸ƒ"""
        errors_only = [
            (pred, true)
            for pred, true, correct in zip(
                self.errors['pred_class'],
                self.errors['true_class'],
                self.errors['is_correct']
            )
            if not correct
        ]

        if len(errors_only) == 0:
            print("   âœ… æ— é”™è¯¯æ¡ˆä¾‹ï¼")
            return

        # æŒ‰ç±»åˆ«ç»Ÿè®¡é”™è¯¯
        error_counts = defaultdict(int)
        for pred, true in errors_only:
            error_counts[f"{self.class_names[true]} â†’ {self.class_names[pred]}"] += 1

        # ç»˜å›¾
        plt.figure(figsize=(12, 6))
        sorted_errors = sorted(error_counts.items(), key=lambda x: x[1], reverse=True)[:15]

        labels, counts = zip(*sorted_errors)
        plt.barh(labels, counts)
        plt.xlabel('é”™è¯¯æ•°é‡')
        plt.title('Top 15 é”™è¯¯ç±»å‹')
        plt.tight_layout()
        plt.savefig(output_dir / 'error_distribution.png', dpi=150)
        plt.close()

    def _analyze_confidence(self, output_dir):
        """ç½®ä¿¡åº¦åˆ†æ"""
        correct_conf = [
            conf for conf, correct in zip(self.errors['confidence'], self.errors['is_correct'])
            if correct
        ]
        wrong_conf = [
            conf for conf, correct in zip(self.errors['confidence'], self.errors['is_correct'])
            if not correct
        ]

        plt.figure(figsize=(10, 6))
        plt.hist(correct_conf, bins=20, alpha=0.5, label='æ­£ç¡®é¢„æµ‹', color='green')
        plt.hist(wrong_conf, bins=20, alpha=0.5, label='é”™è¯¯é¢„æµ‹', color='red')
        plt.xlabel('ç½®ä¿¡åº¦')
        plt.ylabel('æ•°é‡')
        plt.title('ç½®ä¿¡åº¦åˆ†å¸ƒå¯¹æ¯”')
        plt.legend()
        plt.tight_layout()
        plt.savefig(output_dir / 'confidence_analysis.png', dpi=150)
        plt.close()

    def _find_hard_cases(self, top_k=50):
        """æ‰¾å‡ºæœ€éš¾çš„æ¡ˆä¾‹"""
        # å®šä¹‰éš¾åº¦ï¼šé”™è¯¯é¢„æµ‹ä¸”é«˜ç½®ä¿¡åº¦
        hard_cases = []

        for i, (img_id, conf, correct) in enumerate(zip(
                self.errors['image_id'],
                self.errors['confidence'],
                self.errors['is_correct']
        )):
            if not correct:
                difficulty = conf  # é”™è¯¯ä½†é«˜ç½®ä¿¡åº¦ = éš¾ä¾‹
                hard_cases.append({
                    'image_id': img_id,
                    'difficulty': float(difficulty),
                    'confidence': float(conf),
                    'pred_class': int(self.errors['pred_class'][i]),
                    'true_class': int(self.errors['true_class'][i])
                })

        hard_cases.sort(key=lambda x: x['difficulty'], reverse=True)
        return hard_cases[:top_k]

    def _compute_per_class_stats(self):
        """æ¯ä¸ªç±»åˆ«çš„ç»Ÿè®¡"""
        stats = {}

        for cls_id, cls_name in enumerate(self.class_names):
            cls_correct = sum(
                1 for true, pred, correct in zip(
                    self.errors['true_class'],
                    self.errors['pred_class'],
                    self.errors['is_correct']
                )
                if true == cls_id and correct
            )
            cls_total = sum(1 for true in self.errors['true_class'] if true == cls_id)

            stats[cls_name] = {
                'accuracy': cls_correct / cls_total if cls_total > 0 else 0,
                'count': cls_total
            }

        return stats


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    from src.config import DATA_YAML
    import yaml

    with open(DATA_YAML) as f:
        class_names = yaml.safe_load(f)['names']

    analyzer = ErrorAnalyzer(class_names)

    # æ¨¡æ‹Ÿæ·»åŠ é¢„æµ‹ç»“æœ
    # å®é™…ä½¿ç”¨æ—¶ï¼Œåœ¨éªŒè¯å¾ªç¯ä¸­è°ƒç”¨analyzer.add_prediction()

    # ç”ŸæˆæŠ¥å‘Š
    report = analyzer.generate_report()